-- Sqoop Commands 

Sqoop is a tool designed to transfer data between Hadoop and relational databases. 
You can use Sqoop to import data from a relational database management system (RDBMS) such as MySQL or Oracle 
into the Hadoop Distributed File System (HDFS), 
transform the data in Hadoop MapReduce, and then export the data back into an RDBMS.

Behind the scenes, the dataset being transferred is split into partitions 
and map only jobs are launched for each partition with the mappers managing transferring the dataset assigned to it.

Sqoop is a client-side tool, so it needs to be installed only on the client on which you wish to use Sqoop.


Text file is the default format.
With MYSQL, text file is the only format supported;  Avro and Sequence file formatted imports are feasible through other RDBMS -


Lists databases in your mysql database.

$ sqoop list-databases 
--connect jdbc:mysql://<<mysql-server>>/employees 
--username airawat --password myPassword

Lists tables in your mysql database.

$ sqoop list-tables 
	--connect jdbc:mysql://<<mysql-server>>/employees 
	--username airawat --password myPassword


1) import a table from mysql to hdfs ( full table )
			sqoop import --connect jdbc:mysql://localhost/database/emp --username root -p 
			-m 6 
			--table employee 
			--target-dir /hdfs/employee


note ; For Oracle Db use   jdbc:oracle:*//
			
	NOTE :  Rather than repeat the import command along with connection related input required, each time, 
		you can pass an options file as an argument to sqoop.  
		Create a text file, as follows, and save it someplace, locally on the node you are running the sqoop client on.  
		
		sqoop import --option-file sqoopimportfile.txt 
		-m 2 
		--table employee
		--target-dir /hdfs/emp

		
		A samole Data File in HDFS Looks as below 
			$ hadoop fs -cat sqoop-mysql/departments/part-m-00000 | more
						d009,Customer Service
	
	
--split-by is used to distribute the values from table across the mappers uniformly i.e. 
say u have 100 unique records(primary key) and if there are 4 mappers, 
--split-by (primary key column) will help to distribute you data-set evenly among the mappers.

$CONDITIONS is used by Sqoop process, it will replace with a unique condition expression internally to get the data-set. 
If you run a parallel import, the map tasks will execute your query with different values substituted in for $CONDITIONS. 
e.g., one mapper may execute "select bla from foo WHERE (id >=0 AND id < 10000)", 
and the next mapper may execute "select bla from foo WHERE (id >= 10000 AND id < 20000)" and so on.
	
This section is straight from the Apache User Guide.

$Conditions

If you want to import the results of a query in parallel, then each map task will need to execute a copy of the query, 
with results partitioned by bounding conditions inferred by Sqoop. Your query must include the token $CONDITIONS which each 
Sqoop process will replace with a unique condition expression. You must also select a splitting column with --split-by.

Controlling parallelism

Sqoop imports data in parallel from most database sources. You can specify the number of map tasks (parallel processes) to use to perform the import by using the -m or --num-mappers argument. 
Each of these arguments takes an integer value which corresponds to the degree of parallelism to employ. By default, four tasks are used. 
Some databases may see improved performance by increasing this value to 8 or 16. 
Do not increase the degree of parallelism greater than that available within your MapReduce cluster; 
tasks will run serially and will likely increase the amount of time required to perform the import. Likewise, do not increase the degree of parallism higher than that which your database can reasonably support. 
Connecting 100 concurrent clients to your database may increase the load on the database server to a point where performance suffers as a result.

When performing parallel imports, Sqoop needs a criterion by which it can split the workload. 
Sqoop uses a splitting column to split the workload. By default, Sqoop will identify the primary key column (if present) in a table and use it as the splitting column. 
The low and high values for the splitting column are retrieved from the database, and the map tasks operate on evenly-sized components of the total range. 
For example, if you had a table with a primary key column of id whose minimum value was 0 and maximum value was 1000, and 
Sqoop was directed to use 4 tasks, Sqoop would run four processes which each execute 
SQL statements of the form 
SELECT * FROM sometable WHERE id >= lo AND id < hi, with (lo, hi) set to (0, 250), (250, 500), (500, 750), and (750, 1001) in the different tasks.

If the actual values for the primary key are not uniformly distributed across its range, then this can result in unbalanced tasks. 
You should explicitly choose a different column with the --split-by argument. For example, --split-by employee_id. 

Note: Sqoop cannot currently split on multi-column indices. If your table has no index column, or has a multi-column key, then you must also manually choose a splitting column.




	
2)import a table using the where condition to hdfs 
			
			sqoop import --connect jdbc:mysql://localhost/database/emp --username root -p
			-m 6
			--table employee
			--where 'SALARY >5000'
			--target-dir /hdfs/employee
			
3) import only the latest value in the mysql table { incremental Import } to HDFS 

			sqoop import --connect jdbc:mysql://localhost/database/emp --username root -p
			-m 6
			--table employee
			--check-column Empid
			--incremental-mode append 
			--last-value 1532894
			--direct
			--target-dir /hdfs/incrementupload
	
--check-column (col): Specifies the column to be examined when determining which rows to import.
--incremental (mode): Specifies how Sqoop determines which rows are new. Legal values for mode include a)append and b)lastmodified.
--last-value (value): Specifies the maximum value of the check column from the previous import.


			
4) import all the Tables in to Hdfs 

			sqoop import-all-tables --connect jdbc:mysql://localhost/database/emp --username root -p
			- m6 
			--target-die /hdfs/all
			

5) import mysql table into HIVE 
			sqoop import --connect jdbc:mysql://localhost/database/emp --username root -p
			-m 6 
			--hive-import 
			--create-hive-table
			--hive-table sample
			--target-hive /usr/warehouse/samplehive
			--enclosed-by'\"'
			--fields-terminated-by ,
			--escaped-by \ \
			
			
				Need to check for the enclosed option in hive 
	
	"d009","Customer Service"    => enclosed by " 
	
6) Create Partition Table in Hive from Mysql 
			
			sqoop import --connect jdbc:mysql://localhost/database/emp --username root -p
			-m 6 
			--query 'select empid , fname , lname , salary from employee where country ='IND' and $CONDITIONS
			--split by empid 
			--hive-import 
			--creeate-hive-table
			--hive-table samplepartition
			--hive-partition-key country
			--hive-partition-value 'IND'
			--target-dir /usr/warehouse/partition/
			--enclosed-by '\"'
			--fields-terminated-by ','
			--escaped-by \ \
			
				Note2: If the column emp_no is listed in lower case in the query, only null is retrieved.  
				If we swicth the case of this just one field, to EMP_NO, it works fine.
			

sqoop  import --connect jdbc:mysql://localhost/northwind --username root -P -m 2 
--query 'select id ,employee_id, customer_id,ship_name,status_id from orders where ship_city = "New York" AND $CONDITIONS '
--split-by id  --hive-import --create-hive-table --hive-table orders_newyork --hive-partition-key ship_city 
--hive-partition-value 'New York' --target-dir /user/hive/wareouse/orders
--------------------------------------------------------------------------------------------------------------------------------------------
IMPORT IN HIVE USING PARTITION 

orders - ship_city 

partition - Ship_City 
sqoop  import --connect jdbc:mysql://localhost/northwind 
--username root -P -m 2
--query 'select id ,employee_id, customer_id,ship_name,status_id from orders where ship_city = "Miami" AND $CONDITIONS '
--split-by id  --hive-import 
--hive-overwrite 
--hive-table orders 
--hive-partition-key ship_city 
--hive-partition-value 'Miami' 
--target-dir /user/hive/wareouse/ordersnew
--fields-terminated-by '$' --lines-terminated-by '\n' 

sqoop  import --connect jdbc:mysql://localhost/northwind --username root -P -m 2 
--query 'select id ,employee_id, customer_id,ship_name,status_id from orders where ship_city = "New York" AND $CONDITIONS ' 
--split-by id  --hive-import --hive-overwrite 
--hive-table orders 
--hive-partition-key ship_city 
--hive-partition-value 'New York' 
--target-dir /user/hive/wareouse/ordersnew
--fields-terminated-by '$' --lines-terminated-by '\n' 


Folder Structure 

esak@esak:~$ hdfs dfs -ls /user/hive/warehouse/orders
Found 2 items
drwxr-xr-x   - esak supergroup          0 2017-03-19 03:13 /user/hive/warehouse/orders/ship_city=Miami
drwxr-xr-x   - esak supergroup          0 2017-03-19 03:16 /user/hive/warehouse/orders/ship_city=New York



Exports are performed by multiple writers in parallel.
Each writer uses a separate connection to the database; these have separate transactions from one another. 
Sqoop uses the multi-row INSERT syntax to insert up to 100 records per statement. 
Every 100 statements, the current transaction within a writer task is committed, causing a commit every 10,000 rows. 
This ensures that transaction buffers do not grow without bound, and cause out-of-memory conditions. 
Therefore, an export is not an atomic process. 
Partial results from the export will become visible before the export is complete.


Exports may fail for a number of reasons:
Loss of connectivity from the Hadoop cluster to the database (either due to hardware fault, or server software crashes)
Attempting to INSERT a row which violates a consistency constraint (for example, inserting a duplicate primary key value)
Attempting to parse an incomplete or malformed record from the HDFS source data
Attempting to parse records using incorrect delimiters
Capacity issues (such as insufficient RAM or disk space)
If an export map task fails due to these or other reasons, it will cause the export job to fail. The results of a failed export are undefined. Each export map task operates in a separate transaction. Furthermore, individual map tasks commit their current transaction periodically. If a task fails, the current transaction will be rolled back. Any previously-committed transactions will remain durable in the database, leading to a partially-complete export.


	
7) Export HDFS DATA into Mysql 

			sqoop export --connect jdbc:mysql://localhost/sampledb/employee --username root -p 
			-m 6
			--table hdfsexport
			--export-dir /hdfs/table3/

8)  Export only updated data in to the mysql table from hdfs 

			sqoop export --connect jdbc:mysql://localhost/sampledb/employee --username root -p 
			-m 6
			--table hdfsupdate
			--update-key empid
			--update-mode updateonly
			--export-dir /hdfs/exportupdate

			
9)Export Data in upsert mode 

	upsert = if insert doesnot exist  update if exists
	
				sqoop export --connect jdbc:mysql://localhost/sampledb/employee --username root -p
				-m 6 
				--table hdfsup
				--update-key empid
				--update-mode allowinsert
				--export-dir /hdfs/expo/upinsert


				
10)  Export a Hive Table to MYsql 

				sqoop export --connect jdbc:mysql://localhost/sampledb/employee --username root -p
				-m 6 
				-table hiveexptbl
				--export-dir /ussr/warehouse/exphive
				--enclosed-by '\"'

				
11)  Export partitioned Table in to MySQL

				sqoop export --connect jdbc:mysql://localhost/sampledb/employee --username root -p 
				-m 6 
				--table partitionhive
				--export-dir /usr/warehouse/partition/gender
				--enclosed-by '\"'
				
				Note 1: With Sqoop 1.4.2., we need to issue a sqoop statement for every partition individually.  
				Note 2:  In the export, the partition key will not be inserted, 
						 you have to issue an update statement for the same.  
				
	Note 2 :	issue the query in mysql 
				update employees_export_hive set gender='M' where (gender="" or gender is null);
		
	Note 1 :    In the Above Query/Scenario We have 2 Parititioins means GENDER = M GENDER = F so 
				we need to execute the sqoop command 
				
12) Export Normal Hive Table in update mode  to MY SQL 

				sqoop export --connect jdbc:mysql://localhost/sampledb/employee --username root -p 
				-m 6 
				--table updateemp
				--export-dir /usr/warehouse/emp
				--update-key emp_id
				--update-mode updateonly
				--enclosed-by '\"'
				
				
13) Export table from hive to MYSQL upinsert 

				sqoop export --connect jdbc:mysql://localhost/sampledb/employee --username root -p 
				-m 6 
				--table updateemp
				--export-dir /usr/warehouse/emp
				--update-key emp_id
				--update-mode allowinsert
				--enclosed-by '\"'
4b.Update data from HDFS into a table in a relational database

Create emp table tbl in mysql test db

create table emp
(
id int not null primary key,
name varchar(50)
);
vi emp --> create file with below contents

1,Thiru
2,Vikram
3,Brij
4,Sugesh
Move the file to hdfs

hadoop fs -put emp <dir>
Execute the below sqoop job to export the data to the mysql

sqoop export --connect <jdbc connection> \
--username sqoop \
--password sqoop \
--table emp \
--export-dir <dir> \
--input-fields-terminated-by ',';
Verify the data in the mysql table

mysql> select * from emp;

+----+--------+
| id | name   |
+----+--------+
|  1 | Thiru  |
|  2 | Vikram |
|  3 | Brij   |
|  4 | Sugesh |
+----+--------+
update the emp file & move the updated file into hdfs. contents of the updated file

1,Thiru
2,Vikram
3,Sugesh
4,Brij
5,Sagar
Sqoop export for upsert - Update if the key matches else insert.

sqoop export --connect <jdbc connection> \
--username sqoop \
--password sqoop \
--table emp \
--update-mode allowinsert \
--update-key id \
--export-dir <dir> \
--input-fields-terminated-by ',';

Note: --update-mode <mode> - we can pass two arguments "updateonly" - to update the records. this will update the records if the update key matches.
if you want to do upsert (If exists UPDATE else INSERT) then use "allowinsert" mode.
example: 
--update-mode updateonly \ --> for updates
--update-mode allowinsert \ --> for upsert
verify the results:

mysql> select * from emp;
+----+--------+
| id | name   |
+----+--------+
|  1 | Thiru  |
|  2 | Vikram |
|  3 | Sugesh |--> Previous value "Brij"
|  4 | Brij   |--> Previous value "Sugesh"
|  5 | Sagar  |--> new value inserted
+----+--------+
				
14) Read Multiple Mysql Files to HDFS not {IMPORT ALL STATEMENT }

				we need to use shell scripting to acheive this 
				we need to create a seperate txt file which has DBNAME.TABLENAME and 
				we need to input it to the sqoop command 
				
			shell script should be like 

					while read line;
					do 
						DBNAME = 'echo $line | cut -d '.' -f1'
						TableName = 'echo $line | cut -d '.' -f2 '
						
						sqoop import -Dmapreduce.job.queuename=$RM_QUEUE_NAME 
						--connect '$JDBC_URL;databaseName=$DBNAME;username=$USERNAME;password=$PASSWORD' 
						--table $tableName  --target-dir $DATA_COLLECTOR/$tableName  
						--fields-terminated-by '\001' 
						-m 1
						
					done<inputfile


15) Import mysql Table in to HBASE 


16)sqoop mergecommands 

sqoop merge --merge-key language_id --new-data /March/sqoopExamples/updatedEsakLanguage --onto /March/sqoopExamples/Esaklanguage --target-dir /March/sqoopExamples/CombinedLanguage --class-name esak_language --jar-file /tmp/sqoop-esak/compile/521f0a2d8be9cd50e15aa0f782709348/esak_language.jar 
we have to delete the		 old folder stage folder and look like oly one folder is available

17) Sqoop JOb 

sqoop job --create job esakjob1 -- import --conect

sqoop job --list
sqoop job --exec esakjob1

					
					
Use -append flag => it will add additional file in to the same folder 

List Tables       sqoop list-tables --connect jdbc:mysql://localhost/sampledb/emp --username root -p

List Database 	  sqoop list-database --connect jdbc:mysql://localhost/sampledb/emp --username root -p
Columns Command 

we can select only specific column in to hdfs from mysql using the -column command 

					sqoop import --connect jdbc:mysql//localhost/sampleDb/Employee --username root -p 
					-m 1 
					--table sample
					--column 'emp_id , fname , country , salary '
					--target-dir /hdfs/selectivecolumns/tables
					
					
					sqoop eval --connect jdbc:mysql//localhost/sampleDb/Employee --username root -p 
					-m 1 
					-query "show tables;"
					

--Misc---

JOBS 
					sqoop job --create myjob --import --connect jdbc:mysql://localhost/db --username root -p 
					--table employee 
					--m 1
					
To execute a job 
					sqoop job --exec myjob 
					
To view/ List the Job 

					sqoop job --list 
					


					

 Flags Used with Import Source 

		--direct   - we can use mysql connector directly it will increase the performance 
		--boundary-query  “SELECT MIN(EMP_NO), MAX(EMP_NO) from employees”   ==> usually 2 integer value determines up and low value 
		--fetch-size=10000   => record to read at once 
		-z   => Files in hdfs willbe compressed default will gzip ==part0000.gz
		

					
					
					
					
					
Questions 

1)  try the min and max value with some other numbers 

		use the below
		
			sqoop --options-file SqoopImportOptions.txt \
--query 'select EMP_NO,FIRST_NAME,LAST_NAME from employees where $CONDITIONS' \
--boundary-query “SELECT MIN(EMP_NO), MAX(EMP_NO) from employees” \
--split-by EMP_NO \
--direct \
--target-dir /user/airawat/sqoop-mysql/BoundaryQuerySample


